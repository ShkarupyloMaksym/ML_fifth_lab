# -*- coding: utf-8 -*-
"""fifth_lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zNEOehHiCWmxwEkWi_nU2Le7faMIyBWp

# Download with pip
"""

!pip install --upgrade gdown

"""# Imports"""

import pandas as pd
import matplotlib.pyplot as plt
import os
import gdown
import re
from IPython.display import display
import numpy as np

from sklearn.model_selection import train_test_split, ShuffleSplit
# from sklearn.tree import DecisionTreeClassifier, export_graphviz
# from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans

from sklearn.metrics import classification_report, confusion_matrix, silhouette_score
import sklearn.metrics as skm
from sklearn.cluster import AgglomerativeClustering
# from sklearn.preprocessing import StandardScaler
# from sklearn.decomposition import PCA
from tqdm.auto import tqdm
import seaborn as sns

"""# Constants"""

file_url = 'https://drive.google.com/file/d/1IVGHhWUq8ee5YNf0fdy1Rk1C3HXQgxb4/view?usp=drive_link'
data_file_name = 'data.csv'

"""# Some functions"""

def get_alphabet_position(letter):
    letter = letter.lower()
    alphabet = 'абвгґдеєжзиіїйклмнопрстуфхцчшщьюя'
    return alphabet.index(letter) + 1

def convert_drive_link(original_link):
  if "https://drive.google.com/uc?id=" in original_link:
    return original_link
  original_link = original_link.replace('?usp=sharing', '').replace('?usp=drive_link', '')
  pattern = r"https://drive\.google\.com/file/d/([a-zA-Z0-9_-]+)/view"

  matcher = re.match(pattern, original_link)

  if matcher:
    file_id = matcher.group(1)
    converted_link = f"https://drive.google.com/uc?id={file_id}"
    return converted_link
  else:
    raise Exception(f"Not realized Google Drive link format.\nGiven link is {original_link}")
    return None


def install_from_google_drive(link, name, path=None, force_download = False):
  full_path = name
  if path is not None:
    full_path = os.path.join(path, full_path)
  if not force_download:
    if os.path.exists(full_path):
      print('The data already exists')
      return

  print('Start downloading')
  gdown.download(convert_drive_link(link), full_path, quiet=False)
  print('\nDownloading have ended')

"""# Download data"""

install_from_google_drive(file_url, data_file_name)

"""# Task

## Constants
"""



"""## 1. Відкрити та зчитати наданий файл з даними."""

df = pd.read_csv(data_file_name, sep=';')

df.head()

"""## 2. Визначити та вивести кількість записів."""

shape = df.shape
print(f'Кількість записів: {shape[0]}')

"""## 3. Видалити атрибут quality."""

df.drop('quality', axis=1, inplace=True)

"""## 4. Вивести атрибути, що залишилися."""

pd.DataFrame(df.columns)

"""## 5. Використовуючи функцію KMeans бібліотеки scikit-learn, виконати розбиття набору даних на кластери з випадковою початковою ініціалізацією і вивести координати центрів кластерів. Оптимальну кількість кластерів визначити на основі початкового набору даних трьома різними способами:
 - elbow method;
 - average silhouette method;
 - prediction strength method (див. п. 9.2.3 Determining the Number of
Clusters книжки Andriy Burkov. The Hundred-Page Machine Learning
Book).

 Отримані результати порівняти і пояснити, який метод дав кращий результат і чому так (на Вашу думку).

### elbow method + average silhouette method
"""

cluster_num_arr = range(2, 20)

def plot_error_plot(error, title):
  error = np.array(error)
  plt.figure(figsize = (13, 7))
  plt.plot(error[:, 0], error[:, 1], marker='o', markersize=5, mfc='r', mec='r')
  plt.xticks(error[:, 0])
  plt.grid(True)
  plt.title(title)
  plt.show()

error_sqd = []
error_silhouette = []
center_list = []

for i in cluster_num_arr:
  model = KMeans(n_clusters=i, init='random', n_init=1, random_state=42)
  res = model.fit_predict(df)
  error_sqd.append([i, model.inertia_])
  error_silhouette.append([i, silhouette_score(df, res)])
  center_list.append(model.cluster_centers_)


plot_error_plot(error_sqd, 'elbow method')
plot_error_plot(error_silhouette, 'average silhouette method')

for arr in center_list:
  for dot in arr[:, :2]:
    plt.plot(dot[0], dot[1], '.')
  plt.show()

"""### prediction strength method"""

def get_prediction_strength(test_train_pred, test_test_pred, cluster_num):
  d = test_test_pred.shape[0]

  d_arr = np.zeros([d, d])
  for m in range(d):
    for n in range(m+1, d):
      d_arr[m][n] = int(test_train_pred[m] == test_train_pred[n])
      d_arr[n][m] = d_arr[m][n]


  ps = np.array([np.inf] * cluster_num)
  for j in range(cluster_num):
    len_aj = sum(test_test_pred == j)

    if len_aj not in [0, 1]:
      d_sum = np.sum(d_arr[test_test_pred == j, :][:, test_test_pred == j])
      ps[j] = d_sum / (len_aj * (len_aj - 1))

  return min(ps)


def prediction_strength_method_KMeans(df, train_size=0.8, cluster_num_arr=range(1, 21), n_tries=1, random_state=42):
  spliter = ShuffleSplit(n_splits=n_tries, train_size=0.7, random_state=random_state)
  splited_data = spliter.split(df)

  pss = dict([(i, []) for i in cluster_num_arr])

  for train_df_i, test_df_i in tqdm(splited_data, total=n_tries):
    if isinstance(df, np.ndarray):
      train_df, test_df = df[train_df_i], df[test_df_i]
    else:
      train_df, test_df = df.loc[train_df_i], df.loc[test_df_i]

    for i in cluster_num_arr:
      test_model = KMeans(n_clusters=i, init='random', n_init=1, random_state=random_state)
      test_model.fit(test_df)
      test_test_pred = test_model.predict(test_df)

      train_model = KMeans(n_clusters=i, init='random', n_init=1, random_state=random_state)
      train_model.fit(train_df)
      test_train_pred = train_model.predict(test_df)


      pss[i].append(get_prediction_strength(test_train_pred, test_test_pred, i))

  for n in pss:
    pss[n] = np.mean(pss[n])

  return pss

pss = prediction_strength_method_KMeans(df, 0.7, n_tries=5)
plot_error_plot(list(pss.items()), 'prediction strength method')

"""## 6. За раніш обраної кількості кластерів багаторазово проведіть кластеризацію методом k-середніх, використовуючи для початкової ініціалізації метод k-means++.
Виберіть найкращий варіант кластеризації. Який кількісний критерій Ви обрали для відбору найкращої кластеризації?
"""

n_chosen = 3

n_models = 10

coolest_model = [None, 0]
chosen_model = 0
error_sqd = []

for n in range(n_models):
  model = KMeans(n_clusters=n_chosen, init='k-means++', n_init=1, random_state=n)
  model.fit(df)
  error_sqd.append([n + 1, model.inertia_])
  if coolest_model[1] < error_sqd[-1][1]:
    coolest_model[0] = model
    chosen_model = n + 1
    coolest_model[1] = error_sqd[-1][1]

print(f'Top model - {chosen_model}')
print(f'With score - {coolest_model[1]}')

plot_error_plot(error_sqd, 'detect best model')

"""## 7. Використовуючи функцію AgglomerativeClustering бібліотеки scikit-learn, виконати розбиття набору даних на кластери. Кількість кластерів обрати такою ж самою, як і в попередньому методі. Вивести координати центрів кластерів."""

model_a = AgglomerativeClustering(n_clusters=n_chosen).fit(df)

# Getting cluster labels and unique labels
cluster_labels = np.unique(model_a.labels_)

# Computing cluster representatives
cluster_centers = []
for label in cluster_labels:
    cluster_centers.append(np.mean(df[model_a.labels_ == label], axis=0))

print("Cluster Centers:")
for center in cluster_centers:
    print(center)

"""## 8. Порівняти результати двох використаних методів кластеризації."""

a_score = silhouette_score(df, model_a.labels_)
m_score = silhouette_score(df, coolest_model[0].labels_)

a_score

m_score